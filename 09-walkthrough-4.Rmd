---
title: 'Education Dataset Analysis Pipeline: Walk Through #4'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, cache = F)
here::set_here(path = "~/documents/data-science-in-education")
```

## Background

Relationships are important to us. In the case of many research techniques, relationships are&mdash;deservedly&mdash;the focus of analyses. It is not very difficult to imagine *qualitative* techniques to study relationships: One could ask other individuals about who their friends are, why they are their friends, and what they like to do when with them. 

Increasingly, it is also not hard to imagine *quantitative* techniques to study relationships, too. In a way, the same questions that could be used qualitatively can serve as the basis for the quantitative study of relationships. Indeed, social network analysis uses these relations in a range of visualizations as well as statistical models.

## Visualization

Let us first consider visualizations.

<!-- Add example visualization here using ggraph? and ideas for how to create and modify visualizations -->

Visualizations of social networks are interesting and powerful--and increasingly common.

Behind these visualizations, though, there are also statistical models and methods that can help to understand what is going on with respect to particular relationships in a network in additional ways.

## Selection and influence

One way to consider these models and methods is in terms of two *processes* at play in our relationships (cite). These two processes are commonly (though not exclusively) the focus of statistical analyses of networks. In addition to not being exclusive, they do not interact independently: they affect each other reciprocally (Xu, Frank, & Penuel, 2018). They are:

- *Selection*: the processes regarding who chooses to have a relationship with whom
- *Infuence*: the processes regarding how who we have relationships with affects our behavior

While these are complex, they can be studied with the type of data collected from asking people about their relationships (and possibly asking them about or studying their behavior--or measuring some outcome). Happily, the use of these methods has expanded along with **R**: many of the best tools for studying social networks are in the form of long-standing R packages. Additionally, while there are many potential naunces to studying selection and influence, these are models that can fundamentally be carried out with regression, or the linear model (or extensions of it).

In this walkthrough, the influence model is the focus. Nevertheless, we provide some direction for how to carry out selection modeling, too, at the end. 

## An example of influence

In this example, we create some example data that can be used to explore questions about how influence works. Note that Joshua Rosenberg and Sarah Galey initially wrote the following code for a walkthrough shared on Ken Frank's website [here](https://msu.edu/~kenfrank/resources.htm).

### Creating example data in the form of an edgelist 

First, let's create three different data frames. Here is what they should contain:

- A data frame indicating who the *nominator* and *nominee* for the relation (i.e., if Stefanie says that José is her friend, then Stefanie is the nominator and José the nominee) - as well as an optional variable indicating the weight, or strength, of their relation.
    - This data frame and its type can be considered the basis for many types of social network analysis and is a common structure for network data: it is an *edgelist*.
- Data frames indicating the values of some behavior - an outcome - at two different time points.

We use the **simstudy** package to simulate the data (and the **dplyr** package to process the data a bit).

Let's first generate a data frame called `data1`:

```{r}
library(simstudy)
library(dplyr)

set.seed("20190101")

def <- defData(varname = "nominator", dist = "categorical", formula = catProbs(n = 200))
def <- defData(def, varname = "nominee", dist = "categorical", formula = catProbs(n = 200))
def <- defData(def, varname = "relate", dist = "nonrandom", formula = 1)

data1 <- genData(500, def)
```

`data2`:

```{r}
unique_nominators <- unique(data1$nominator) # we need these to determine how many rows to generate
unique_nominees <- unique(data1$nominee)

def <- defData(varname = "yvar1", dist = "normal", formula = 2, variance = 2)

data2 <- genData(length(unique_nominees), def)
data2 <- data2 %>% 
    mutate(nominee = unique_nominees) %>% 
    select(nominee, yvar1)
```

And `data3`:

```{r}
def <- defData(varname = "yvar2", dist = "normal", formula = 5, variance = 2)

data3 <- genData(length(unique_nominators), def)
data3 <- data3 %>% 
    mutate(nominator = unique_nominators) %>% 
    select(nominator, yvar2)
```

### Joining the data

Next, we'll join the data into one data frame. Note that while this is sometimes tedius and time-consuming, especially with large sources of network data, it is a key step for being able to carry out network analysis - often, even for creating visualizations that are informative.

```{r}
library(dplyr)

data <- left_join(data1, data2, by = "nominee")
data$nominee <- as.character(data$nominee) # this makes merging later easier

# calculate indegree in tempdata and merge with data
tempdata <- data.frame(table(data$nominee)) # this needs to be calculated differently
names(tempdata) <- c("nominee", "indegree") # rename the column "nominee"
tempdata$nominee <- as.character(tempdata$nominee) # makes nominee a character data type, instead of a factor, which can cause problems
data <- left_join(data, tempdata, by = "nominee")
```

### Creating the Network Graph

Now, we'll use the **igraph** package to create a *graph* of our simulated data's network. Once we have this graph, we can calculate many useful descriptive statistics associated with the network's features:  

- Diameter: the length of the longest geodesic (max distance between two vertices)
- Density: 
- Transitivity: The balance of connections. Also called the clustering coefficient. The probability that the adjacent vertices of a vertex are connected. When the clustering coefficient is large it implies that a graph is highly clustered around a few nodes; when it is low it implies that the links in the graph are relatively evenly spread among all the nodes (Hogan, 2017).

- Reciprocity: The proportion of mutual connections (in a directed network). The probability that the opposite counterpart of a directed edge is also included in the graph.

- Degree: 


```{r}
library(igraph)
sim_graph <- data %>% select(nominator, nominee) %>%  # this creates an edgelist
        as.matrix %>% 
        graph_from_edgelist(directed=TRUE) %>%
        set_vertex_attr(name='degree', value=degree(., mode='total', loops=FALSE)) %>% 
        set_vertex_attr(name='in_degree', value=degree(., mode='in', loops=FALSE)) %>% 
        set_vertex_attr(name='out_degree', value=degree(., mode='out', loops=FALSE))

network_summary <- sim_graph %>% V %>% length %>%  # number of vertices/nodes
        rbind(sim_graph %>% gsize) %>%  # number of edges
        rbind(sim_graph %>% diameter) %>%  # max distance between two vertices
        rbind({sim_graph %>% edge_density * 100} %>% round(2)) %>%     
        rbind({sim_graph %>% transitivity("global") * 100} %>% round(2)) %>% 
        rbind({sim_graph %>% reciprocity * 100} %>% round(2))  %>%
        rbind(sim_graph %>% vertex_attr('degree') %>% mean %>% round(2)) %>% 
        rbind(sim_graph %>% vertex_attr('degree') %>% sd %>% round(2)) %>% 
        rbind(sim_graph %>% vertex_attr('degree') %>% median) %>% 
        rbind(sim_graph %>% vertex_attr('degree') %>% min) %>%
        rbind(sim_graph %>% vertex_attr('degree') %>% max)
    
colnames(network_summary) <- c("")
rownames(network_summary) <- c("Number of nodes: ", "Number of edges: ", "Diameter: ",
                               "Density: ", "Transitivity: ", "Reciprocity: ",
                               "Mean degree: ", "SD degree: ", "Median degree: ",
                               "Min degree: ", "Max degree: ")
network_summary

```

### Clustering

With a graph of our simulated network, we can see if *clustering* occurs in this network. If so, what are some characteristics of these clusters?  

First, a definition: a cluster (also called a *community*) is a set of nodes with many edges inside the community and few edges between outside it (i.e. between the community itself and the rest of the graph). There are numerous methods for determining network clusters, but here we use the *spinglass clustering algorithm*, which maps community detection onto finding the ground state of an infinite range spin glass. Csardi, Nepusz, and Airoldi (2016, pp. 132-133) explained:

>
The clustering method of [Reichardt and Bornholdt](https://arxiv.org/abs/cond-mat/0603718) (2006) is motivated by spin glass models from statistical physics. Such models are used to describe and explain magnetism at the microscopic scale at finite temperatures. Reichardt and Bornholdt (2006) drew an analogy between spin glass models and the problem of community detection on graphs and proposed an algorithm based on the simulated annealing of the spin glass model to obtain well-defined communities in a graph. A spin glass model consists of a set of particles called spins that are coupled by ferromagnetic or antiferromagnetic bonds. Each spin can be in one of k possible states. The well-known Potts model then defines the total energy of the spin glass with a given spin configuration... Spins and interactions in the Potts model are very similar to graphs: each spin in the model corresponds to a vertex, and each interaction corresponds to an edge... Reichardt and Bornholdt (2006) gave efficient update rules for the above energy function, making it possible to apply a simulated annealing procedure to find the ground state of the model that corresponds to a low energy configuration. Their algorithm starts from a random configuration of spins and tries to flip all the spins once in each time step. After each individual spin flip, the energy of the new configuration is evaluated.
>

In other words, the spinglass clustering algorithm partitions nodes into communities by optimizing an energy function. The energy is optimized using the following function (Reichardt and Bornholdt, 2008): 
$$H({\sigma}) = -\sum(a_{ij} \textrm{internal links}) + \sum(b_{ij}\textrm{internal non-links}) + \sum(c_{ij}\textrm{external links}) - \sum(d_{ij}\textrm{external non-links})$$. 

This function penalizes missing edges or non-links of people/nodes in the same cluster and present links or edges between people/nodes in different clusters. It also rewards present links or edges between people/nodes in the same cluster and missing links or edges between people/nodes in different clusters. Thus, a lower score (i.e., lower energy level) is better as it means that the internal links and external non-links have more weightage in that model. In other words, in a strong model, members within clusters are strongly linked and members in separate clusters are weakly linked . Here, $a_{ij}, b_{ij}, c_{ij}, d_{ij}$ represent the individual weights of the four components. 

The initial R code to produce spinglass clusters is straightforward:

```{r}
library(igraph)
sim_membership <- clusters(sim_graph) %>% as.data.frame %>% pull(membership)

sim_graph <- sim_graph %>% 
        set_vertex_attr(name='membership', 
                        value = clusters(sim_graph) %>% 
                            as.data.frame %>% pull(membership)
                        )
sim_giant <- sim_graph %>% delete_vertices({vertex_attr(., 'membership') != 1} %>% which)


csg_0 <- sim_giant %>% cluster_spinglass  # creates the clusters; 'csg' = cluster spinglass
csg_0$membership %>% unique %>% length  # number of clusters/communities/groups
```

One of the important outcomes of this method is the _modularity_ value $M$. Modularity measures how good the division is, or how separated are the different vertex types from each other. The spinglass algorithm looks for the modularity of the optimal partition. For a given network, the partition with maximum modularity corresponds to the optimal community structure (i.e., a higher $M$ is better).

Note also that if $M$ = 0, all nodes belong to one group; if $M$ < 0, each node belongs to separate community. 

```{r modularity, include=TRUE}
csg_0$modularity  # The modularity of a graph with respect to some division (or vertex types) 
```

*Identifying the "Typical" Number of Clusters Returned with the Spinglass Algorithm*

It is important to note that a different result is returned each time the spinglass clustering algorithm is run. For this reason, we needed to run a number of simulations to see what the "typical" number of clusters are. We ran the algorithm 100 times and looked at the mean and median number of clusters obtained. We made a note of a _seed_ that produced the median number of clusters, confirmed that this was reproducible, and then set this seed so that all future work will be run with this same clustering configuration.

```{r}
csg_matrix <- matrix(NA, nrow=1, ncol=100)
for (i in 1:100) {
        print(i)
        set.seed(i)
        csg = sim_giant %>% cluster_spinglass
        csg_matrix[1,i] <- max(csg$membership)
}

csg_matrix_summary <- csg_matrix %>% length %>%
        rbind(csg_matrix %>% mean %>% round(2)) %>% 
        rbind(csg_matrix %>% sd %>% round(2)) %>% 
        rbind(csg_matrix %>% median) %>% 
        rbind(csg_matrix %>% min) %>%
        rbind(csg_matrix %>% max)
colnames(csg_matrix_summary) <- c("")
rownames(csg_matrix_summary) <- c("number of tests: ", "mean: ", "sd: ",  "median: ", "min: ", "max: ")
csg_matrix_summary
```

```{r}
## select a seed from this list which reproduces the median number of clusters
seeds <-{as.vector(csg_matrix) == median(csg_matrix)} %>% which
our_seed <- seeds[1]
set.seed(our_seed)  # set the seed
csg <- sim_giant %>% cluster_spinglass

csg_summary <- csg$vcount %>% 
        rbind(sim_giant %>% gsize) %>% 
        rbind(csg$csize %>% length) %>% 
        rbind(csg$modularity %>% round(4))
colnames(csg_summary) <- c("")
rownames(csg_summary) <- c("Number of nodes: ", "Number of edges: ", "Number of clusters: ", "Modularity: ")
csg_summary

print("Size of each cluster: ", quote=FALSE); print(csg$csize)
```

*Test of Statistical Significance for Spinglass Clusters*

The test for statistical significance for spinglass clustering is a bit different than the familiar tests that return $p$-values (Csardi, Nepusz, & Airoldi (2016, pp. 132-138).

The idea behind this test of significance is that a random network of equal size and degree distribution as our observed network should have a lower modularity score--that is, if the observed network does in fact have statistically significant clustering.

The following R procedure generates 100 randomized instances of our network (with the same size and degree distribution) using the `sample_ degseq()` function. The `method = 'vl'` ensures that there are no loop edges in the randomly generated networks. We then applied the spinglass clustering algorithm to each of the 100 randomized instances of the network.

A '0' result from this procudure indicates that no randomized networks have community structure with a modularity score that is higher than the one obtained from the original, observed network. Hence a '0' result means that our network has significant community structure; any non-zero results means that the detected spinglass clusters are not statistically significant.

```{r spinglass_sig_test, include=TRUE}
degrees <- sim_giant %>% as.undirected %>% degree(mode='all', loops=FALSE)
qr_vl <- replicate(100, sample_degseq(degrees, method="vl"), 
                   simplify=FALSE) %>%
        lapply(cluster_spinglass) %>%
        sapply(modularity) 
sum(qr_vl > csg$modularity) / 100
```

### Network Visualization with Clusters

Finally, we created a visualization of our network structure, using the color palette generated by our spinglass clustering. Here, we used the _Fruchterman-Reingold layout algorithm_ (`layout = 'fr'`), which is appropriate for large (but still with less than 1,000 nodes), potentially disconnected networks.

```{r set_cluster_palette, include=FALSE}
## color-blind palette
## source: https://jacksonlab.agronomy.wisc.edu/2016/05/23/15-level-colorblind-friendly-palette/
palette <- c("#000000","#004949","#009292","#ff6db6","#ffb6db",
             "#490092","#006ddb","#b66dff","#6db6ff","#b6dbff",
             "#920000","#924900","#db6d00","#24ff24","#ffff6d")
pie(rep(1,15), col=palette)
csg_palette <- palette[csg$membership]
```



```{r network_visualization, include=TRUE, echo=FALSE}
library(ggraph)
sim_giant_weighted <- sim_giant %>%
        set_edge_attr(name='cluster_weight', 
                      value=ifelse(igraph::crossing(csg, sim_giant), 1, 100))
sim_giant_weighted %>% summary

layout_auto <- sim_giant %>% create_layout(layout='nicely')
layout_fr <- sim_giant %>% create_layout(layout='fr')
layout_drl <- sim_giant %>% create_layout(layout='drl')
layout_kk <- sim_giant %>% create_layout(layout='kk')
layout_lgl <- sim_giant %>% create_layout(layout='lgl')
layout_dh <- sim_giant %>% create_layout(layout='dh')
layout_mds <- sim_giant %>% create_layout(layout='mds')
layout_graphopt <- sim_giant %>% create_layout(layout='graphopt')
layout_sugiyama <- sim_giant %>% create_layout(layout='sugiyama')
layout_randomly <- sim_giant %>% create_layout(layout='randomly')

ggraph(layout_auto) +
        geom_edge_link(width=.1, arrow = arrow(length=unit(1, 'mm'))) +
        geom_node_point(alpha=.75, 
                        size=5, 
                        color=csg_palette
                        ) +
        theme_bw() +
        theme(plot.background = element_blank(),
              panel.grid.major = element_blank(),
              panel.grid.minor = element_blank(),
              panel.border = element_blank(),
              axis.title = element_blank(),
              axis.text = element_blank(),
              axis.ticks = element_blank(),
              legend.position="none"
        )
#ggsave("model-output/network_visualization.png", width = 1 * 10, height = 1 * 10)
```










<<<<<<< HEAD
Let's take a look at the merged data. What this data now contains is the first data frame, `data1`, with each nominees' outcome at time 1 (`yvar1`). Note that we will find each nominators' outcome at time 2 later on.
=======
Let's take a look at the merged data.
>>>>>>> 2c0cdf5fc112012b0fb8b8b0993f3fbe7224bd46

```{r}
data <- as_tibble(data)
data
```

### Calculating an exposure term

This is the key step that makes this model - a regression, or linear, model - one that is special. It is creating an exposure term. The idea is that the exposure term "captures" how your interactions with someone, over some period of time (between the first and second time points) impact some outcome. This model accounts for an individual's initial report of the outcome, i.e., their time 1 prior value, so it is a model for *change* in some outcome.

```{r}
<<<<<<< HEAD
# Calculating exposure
=======
# Calculating exposure and an exposure term that uses indegree, exposure_plus
>>>>>>> 2c0cdf5fc112012b0fb8b8b0993f3fbe7224bd46
data$exposure <- data$relate * data$yvar1

# Calculating mean exposure
mean_exposure <- data %>%
    group_by(nominator) %>%
    summarize(exposure_mean = mean(exposure))
<<<<<<< HEAD
=======

mean_exposure_plus <- data %>%
    group_by(nominator) %>%
    summarize(exposure_plus_mean = mean(exposure_plus))
>>>>>>> 2c0cdf5fc112012b0fb8b8b0993f3fbe7224bd46
```

What this data frame - `mean_exposure` - contains is the mean of the outcome (in this case, `yvar1`) for all of the individuals the nominator had a relation with.

As we need a final data set with `mean_exposure`, `mean_exposure_plus`, `degree`, `yvar1`, and `yvar2` added, we'll process the data a bit more.

```{r}
mean_exposure_terms <- left_join(mean_exposure, mean_exposure_plus, by = "nominator")

names(data2) <- c("nominator", "yvar1") # rename nominee as nominator to merge these
final_data <- left_join(mean_exposure_terms, data2, by = "nominator")
final_data <- left_join(final_data, data3, by = "nominator") # data3 already has nominator, so no need to change
```

### Regression (linear models)

Calculating the exposure term is the most distinctive and important step in carrying out influence models. Now, we can simply use a linear model to find out how much relations - as captured by the influence term - affect some outcome.

```{r}
model1 <- lm(yvar2 ~ yvar1 + exposure_mean, data = final_data)
summary(model1)
```

Note that these models show ...

<!-- Will want to interpret using the (needs-to-be) simulated data -->

So, the influence model is used to study a key process for social network analysis, but it is one that is useful, because you can quantify, given what you measure and how you measure it, *the network effect*, something that is sometimes not considered, especially in education (Sweet, 2017). It's also fundamentally a regression. That's really it, as the majority of the work goes into calculating the exposure term.

## Selection models

While this tutorial focused on influence models, selection models are also commonly used - and are commonly of interest not only to researchers but also to administrators and teachers (and even to youth and students). 

Here, we briefly describe a few possible approaches for using a selection model.

At its core, the selection model is a regression - albeit, one that is a generalization of one, namely, a logistic regression (sometimes termed a generalized linear model, because it is *basically* a regression but is one with an outcome that consists just of 0's and 1's). Thus, the most straight-away way to use a selection model is to use a logistic regression where all of the relations (note the `relate` variable in `data1` above) are indicated with a 1. But, here is the important and challenging step: all of the *possible relations* (i.e., all of the relations that are possible between all of the individuals in a network) are indicated with a 0 in an edgelist. Note that, again, an edgelist is the preferred data structure for carrying out this analysis. This step involves some data wrangling, especially the idea of widening or lengthening a data frame.

<!-- May want to add a short bit of code on this using `gather()` and `spread()` -->

Once all of the relations are indicated with a 1 or a 0, then a simple linear regression can be used. Imagine that we are interested in whether individuals from the *same* group are more or less likely to interact than those from different groups; same could be created in the data frame based upon knowing which group both nominator and nominee are from:

```{r, eval = FALSE}
m_selection <- glm(relate ~ 1 + same, data = edgelist1)
```

While this is a straightforward way to carry out a selection model, there are some limitations to it. Namely, it does not account for individuals who send more (or less) nominations overall--and not considering this may mean other effects, like the one associated with being from the *same* group, are not accurate. A few extensions of the linear model - including those that can use data for which relationships are indicated with weights, not just 1's and 0's, have been developed. 

One type of model extends the logistic regression. It can be used for data that is not only 1's and 0's but also data that is normally distributed or has fixed-ranks. It is the **amen** package available [here](https://cran.r-project.org/web/packages/amen/index.html).

A particularly common one is an Exponential Random Graph Model, or an ERGM. An R package that makes estimating these easy is available [here](https://cran.r-project.org/web/packages/ergm/index.html). That R package, **ergm**, is part of a powerful and often-used collection of packages, including those for working with network data (data that can begin with an edgelist, but may need additional processing that is challenging to do with edgelist data), **statnet**. A link to the statnet packages is [here](https://statnet.org/).