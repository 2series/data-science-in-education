# 16-walkthrough-sna

In the past, if a teacher wanted advice about how to plan a unit or to design a lesson, they would likely turn to a trusted peer in their building or district (Spillane, Kim, Chong Min, & Frank, 2012). In the present, though, they are as likely to turn to someone in the professional learning network (Trust, Krutka, & Carpenter, 2016). 

There are a few reasons to be interested in social media. For example, if you work in a school district, you may be interested in who is interacting with the content you share. If you are a researcher, you may wish to investigate what teachers, administrators, and others do through state-based hashtags (e.g., Rosenberg, Greenhalgh, Koehler, Hamilton, & Akcaoglu, 2016). Social media-based data can also be interesting because it provides new contexts for learning to take place, such as learning through informal communities.

In this chapter, we focus on a source of data that could be used to understnad how one new community functions. That community, #tidytuesday is one sparked by the work of one of the *Data Science in Education Using R* co-authors, Jesse Mostipak, who created the #r4ds community from which #tidytuesday was created. #tidytuesday is a weekly data visualization challenge. A great place to see examples from past #tidytuesday challenges is [this interactive Shiny application](https://github.com/nsgrantham/tidytuesdayrocks). In this walkthrough, we focus on a) accessing data on #tidytuesday from Twitter and b) trying to understand the nature of the interactions that take place through #tidytuesday. We note that while we focused on #tidytuesday because we think it exemplifies the new kinds of learning that a data science toolkit allows an analyst to try to understand (through new data sources), we also chose this because it is straightforward to access data from Twitter, and we think you may find other opportunities to analyze data from Twitter in other cases.

## Accessing data

In this chapter, we access data using the rtweet package (Hawksey, 2019). Through rtweet, it is easy to access data from Twitter as long as one has a Twitter account. We will load the tidyverse and rtweet packages to get started. Here is an example of searching the most recent 1,000 tweets which include the hashtag #rstats. When you run this code, you will be prompted to authenticate your access via Twitter. 

```{r}
library(tidyverse)
library(rtweet)
```

```{r, eval = FALSE}
rstats_tweets <- search_tweets("#rstats")
```

The search term can be easily changed to other hashtags - or other terms. To search for #tidytuesday tweets, we can simply replace #rstats with #tidytuesday. 

```{r, eval = FALSE}
tidytuesday_tweets <- search_tweets("#tidytuesday")
```

A key point--and limitation--for how Twitter allows access to their data for the seven most recent days. There are a number of ways to access older data, which we discuss at the end of this chapter, though we focus on one way here: having access to the URLs to (or the status IDs for) tweets. We used this technique, which we describe in this chapter's Technical Appendix, along with other strategies for collecting historical data from Twitter. The data that we processed is available in the dataedu R package as the `tt-tweets` dataset.

```{r}
library(dataedu)
tt_tweets
```

# Preparing the data for the analysis

Network data, in general, and network data from Twitter, particularly, requires some processing before it can be used in subsequent analyses. In particular, we are going to create an edgelist, a data structure that is especially helpful for understanding the nature of relationships. 

An edgelist looks like the following, where the sender denotes who is initiating the interaction or relationship, and the receiver is who is the recipient of it:

```{r, include = FALSE}
library(randomNames)

names_d1 <- randomNames(6) %>% 
  as_tibble() %>% 
  mutate(sender = 1:6) %>% 
  set_names(c("sender2", "sender"))

names_d2 <- randomNames(6) %>% 
  as_tibble() %>% 
  mutate(receiver = 1:6) %>% 
  set_names(c("receiver2", "receiver"))

example_edgelist <- tibble(sender = c(2, 1, 3, 1, 2, 6, 3, 5, 6, 4, 3, 4), 
                           receiver = c(1, 2, 2, 3, 3, 3, 4, 4, 4, 5, 6, 6))

example_edgelist <- example_edgelist %>% 
  left_join(names_d) %>% 
  left_join(names_d2) %>% 
  select(sender = receiver2, receiver = value)
```

```{r, echo = FALSE}
example_edgelist
```

In this edgelist, the sender could indicate, for example, someone who nominates someone else (the receiver) as someone they go to for help. The sender could also indicate someone who interacted with the receiver, such as by recognizing one of their tweets with a favorite (or retweet). In the following steps, we will work to create an edgelist from the data from #tidytuesday on Twitter.

## Extracting retweets

First, let's start with retweets. We'll identify retweets on the basis of the tweet beginning with the pattern "RT @". There is a lot going on in the code below; let's break it down line-by-line, starting with the `mutate()`:

- `first_four_chars = str_sub(text, start = 1, end = 4),`: this line of code identifies the first four characters of the tweet
- `is_retweet = str_detect(first_four_chars, "RT @"),`: this line of code then determines whether the first four characters - identified through the last line of code - begin with "RT @"
- `retweeted_username = ifelse(is_retweet, str_extract(text, regex), NA))`: this line of code determine if a tweet is a retweet - identified through the very last line of code - and, then, if it is, extracts the *first* username (note that `str_extract()`, instead of `str_extract_all()`, is used for this purpose), using the same regex used earlier

We'll use a regular expression, or regex, to do so.

Find one of the regular expression (or regex) patterns that are in an answer on this page; all should work, but some work a bit easier than others: https://stackoverflow.com/questions/18164839/get-twitter-username-with-regex-in-r

```{r}
regex <- ""

tt_tweets <- tt_tweets %>% 
  mutate(first_four_chars = str_sub(text, start = 1, end = 4),
         is_retweet = str_detect(first_four_chars, "RT @"),
         retweeted_username = ifelse(is_retweet, str_extract(text, regex), NA))
```

Let's put the retweets in their own data frame, called `retweets`.

```{r}
retweets <- tt_tweets %>% 
  filter(is_retweet) %>% 
  select(from_user, retweeted_username) %>% 
  mutate(retweeted_username = str_trim(retweeted_username))
```

The very last line `mutate(all_mentions = str_trim(all_mentions))` - is only necessary if the regex you used left an extra space at the beginning or at the end of the screen name. If you don't have an extra space, it won't hurt, though.

## Extracting mentions

Next, let's get the mentions. Start by replacing `tt_tweets` with the name of your data (the one you read in in the previous step). 

```{r}
tt_tweets <- tt_tweets %>% 
  mutate(all_mentions = str_extract_all(text, regex)) %>% 
  mutate(has_mention = ifelse(!is.na(all_mentions), TRUE, FALSE)) %>% 
  unnest(all_mentions)
```

Let's put these into their own data frame, too.

```{r}
mentions <- tt_tweets %>% 
  filter(has_mention) %>% 
  mutate(all_mentions = str_trim(all_mentions)) %>% 
  select(from_user, all_mentions)
```

## Putting the edgelist together

Now, we are ready to put the pieces together, in order to construct an edgelist. An edgelist is a common social network analysis data structure that has columns for the "sender" and "receiver" of interactions, or relations. For example, we can consider the person retweeting someone else "sending" a retweet to the person who is retweeted, who "receives" the retweet. For mentions, someone "sends" the mention to someone who is mentioned, who can be considered to "receive" it. This will require one last processing step.

Take a look at each data frame we created; type `mentions` and `retweets`, below:

```{r}
retweets
```

```{r}
mentions
```

What needs to happen to these to make them easier to work with in an edgelist? One step is to remove the "@" symbol from the columns we created - `retweeted_username` and `all_replies`.

Let's do this for retweets.

```{r}
retweets <- retweets %>% 
  mutate(retweeted_username = str_sub(retweeted_username, start = 2)) %>%
  mutate(interaction_type = "retweet") %>% # this is so we can distinguish between retweets and mentions
  select(sender = retweeted_username, receiver = from_user, interaction_type) # this is so the columns have the same names for when we combine them back together
```

Almost there! Let's do the same for mentions; write code that will do this, starting with the `mentions` data frame, using the code above as an example. Consider which is the receiver and which is the sender for mentions.

```{r}
mentions <- mentions %>% 
```

Finally, let's create an edgelist:

```{r}
edgelist <- bind_rows(retweets, mentions)
edgelist
```

# Plotting the network

We'll use the **tidygraph** and **ggraph** packages to visualize the data.

```{r}
# install.packages(c("tidygraph", "ggraph"))
library(tidygraph)
library(ggraph)
```
We'll use the `as_tbl_graph()` function, which (by default) identified the first column as the "sender" and the second as the "receiver." Let's look at the object it creates, too.

```{r}
g <- as_tbl_graph(edgelist)
g
```

Next, we'll use the `ggraph()` function. Run the code below, and then uncomment, one at a time, the next two lines (the two beginning `geom_()`, running the code after uncommenting each line).

```{r}
g %>% 
  ggraph() +
  geom_node_point() +
  # geom_node_text(aes(label = name)) +
  # geom_edge_link() +
  theme_graph()
```

Finally, lets size the points based on a measure of centrality, typically a measure of how (potentially) influence an individual may be, based on the interactions observed.

```{r}
g %>% 
  mutate(centrality = centrality_authority()) %>% 
  ggraph() +
  geom_node_point(aes(size = centrality, color = centrality)) +
  scale_color_continuous(guide = 'legend') + 
  geom_node_text(aes(label = name)) +
  geom_edge_link() +
  theme_graph()
```

There is much more you can do with **ggraph** (and **tidygraph**); check out the **ggraph** tutorial here: https://ggraph.data-imaginist.com/

## Social network analysis models

## Selection and influence

Behind these visualizations, though, there are also statistical models and methods that can help to understand what is going on with respect to particular relationships in a network in additional ways.

One way to consider these models and methods is in terms of two *processes* at play in our relationships (cite). These two processes are commonly (though not exclusively) the focus of statistical analyses of networks. In addition to not being exclusive, they do not interact independently: they affect each other reciprocally (Xu, Frank, & Penuel, 2018). They are:

- *Selection*: the processes regarding who chooses to have a relationship with whom
- *Infuence*: the processes regarding how who we have relationships with affects our behavior

While these are complex, they can be studied with the type of data collected from asking people about their relationships (and possibly asking them about or studying their behavior--or measuring some outcome). Happily, the use of these methods has expanded along with R: many of the best tools for studying social networks are in the form of long-standing R packages. Additionally, while there are many potential naunces to studying selection and influence, these are models that can fundamentally be carried out with regression, or the linear model (or extensions of it).

While these are beyond the focus of this walkthrough, the following may be helpful resources for learning more:

- For selection models, the [amen](https://cran.r-project.org/web/packages/amen/index.html) and [ergm](https://cran.r-project.org/web/packages/ergm/index.html) package

- For influence, see Ken Frank's workshop materials on influence ([here](https://msu.edu/~kenfrank/resources.htm))

## Technical Appendix

### Accessing historical data using status URLs

Because the creator of the interactive web application for exploring #tidytuesday content, #tidytuesday.rocks, searched for (and archived) #tidytuesday tweets on a regular basis, a large data set from more than one year of weekly #tidytuesday challenges is available through [the GitHub repository](https://github.com/nsgrantham/tidytuesdayrocks) for the Shiny application. These Tweets (saved in the `data` directory) can be read with the following function

```{r}
raw_tidytuesday_tweets <- read_delim("https://raw.githubusercontent.com/nsgrantham/tidytuesdayrocks/master/data/tweets.tsv", "\t", escape_double = FALSE, 
                                     trim_ws = TRUE)
```

Then the URL for the tweet (the `status_url` column) can be passed to a different rtweet function than the one we used, `lookup_statuses()`. Before we do this, there is one additional step to take. Because most of the Tweets are from more than seven days ago, Twitter requires an additional authentication step. In short, you need to use keys and tokens for the Twitter API, or application programming interface. This [rtweet vignette on accessing keys and tokens](https://rtweet.info/articles/auth.html) explains the process. The end result will be that you will create a token using rtweet that you will use along with your rtweet function (in this case, `lookup_statuses()`):

```{r, eval = FALSE}
token <- create_token(consumer_key = <add-your-key-here>,
                      consumer_secret = <add-your-secret-here>)

# here, we pass the status_url variable from raw_tidytuesday_tweets as the statuses to lookup in the lookup_statuses() function, as well as our token
tidytuesday_tweets <- lookup_statuses(raw_tidytuesday_tweets$status_url,
                                      token = token)
```

The end result will be a tibble, like that above for #rstats, for #tidytuesday tweets.

### Accessing historical data when you do not have access to status URLs

In the above case, we had access to the URLs for tweets because they were saved for the #tidytuesday.rocks Shiny. But, in many cases, historical data will not be available. There are two strategies that may be helpful.

First is [TAGS](https://tags.hawksey.info/). TAGS is based in, believe it or not, Google Sheets, and it works great for collecting Twitter data over time - even a long period of time The only catch is that you need to setup and start to use a TAGS sheet *in advance of the period for which you want to collect data*. For example, you can start a TAGS archiver in August of one year, with the intention to collect data over the coming academic year; or, you can start a TAGS archiver before an academic conference for which you want to collect Tweets.

A second option is the Premium API through Twitter(see [here](https://developer.twitter.com/en/premium-apis)). This is an expensive option, but is one that can be done through [rtweet](https://rtweet.info/reference/search_fullarchive.html), and can also access historical data, even if you haven not started a TAGS sheet and do not otherwise have access to the status URLs.

## References

Rosenberg, J. M., Greenhalgh, S. P., Koehler, M. J., Hamilton, E., & Akcaoglu, M. (2016). An investigation of State Educational Twitter Hashtags (SETHs) as affinity spaces. E-Learning and Digital Media, 13(1-2), 24-44. http://dx.doi.org/10.1177/2042753016672351

Spillane, J., Kim, Chong Min,Frank, K.A. 2012. “Instructional Advice and Information Providing and Receiving Behavior in Elementary Schools: Exploring Tie Formation as a Building Block in Social Capital Development.” American Educational Research Journal. Vol 49 no. 6 1112-1145

Trust, T., Krutka, D. G., & Carpenter, J. P. (2016). “Together we are better”: Professional learning networks for teachers. Computers & education, 102, 15-34.