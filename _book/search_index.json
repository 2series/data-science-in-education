[
["education-dataset-analysis-pipeline-walkthrough-1.html", "6 Education Dataset Analysis Pipeline: Walkthrough #1 6.1 Background and Purpose 6.2 Processing the data 6.3 Viewing the data 6.4 Processing the pre-survey data 6.5 Processing the course data 6.6 Joining the data 6.7 Finding distinct cases at the student-level 6.8 Visualizations and Models 6.9 Linear model (regression) 6.10 What is next?", " 6 Education Dataset Analysis Pipeline: Walkthrough #1 6.1 Background and Purpose In the 2015-2016 and 2016-2017 school years, researchers carried out a study on students’ motivation to learn in online science classes. The online science classes were part of a statewide online course provider designed to supplement(and not replace) students’ enrollment in their local school. For example, students may choose to enroll in an online physics class because one was not offered at their school (or they were not able to take it given their schedule). The study involved a number of different data sources which were explored to understand students’ motivation: A self-report survey for three distinct but related aspects of students’ motivation Log-trace data, such as data output from the learning management system Discussion board data (not used in this walkthrough) Achievement-related (i.e., final grade) data Our purpose for this walkthrough is to begin to understand what explains students’ performance in these online courses. To do so, we will focus on a variable that was available through the learning management system used for the courses, on he amount of time sudents’ spent on the course. We will also explore how different (science) subjects as well as being in a particular class may help to explain student performance. First, these different data sources will be described in terms of how they were provided by the school. 6.1.1 Data Source #1: Self-report survey This was data collected before the start of the course via self-report survey. The survey included 10 items, each corresponding to one of three measures, namely, for interest, utility value, and perceived competence: I think this course is an interesting subject. (Interest) What I am learning in this class is relevant to my life. (Utility value) I consider this topic to be one of my best subjects. (Perceived competence) I am not interested in this course. (Interest - reverse coded) I think I will like learning about this topic. (Interest) I think what we are studying in this course is useful for me to know. (Utility value) I don’t feel comfortable when it comes to answering questions in this area. (Perceived competence) I think this subject is interesting. (Interest) I find the content of this course to be personally meaningful. (Utility value) I’ve always wanted to learn more about this subject. (Interest) 6.1.2 Data source #2: Log-trace data Log-trace data is data generated from our interactions with digital technologies, such as archived data from social media postings (see Chapter XXX and XXX). In education, an increasingly common source of log-trace data is that generated from interactions with learning management systems and other digital tools (Lazer et al., 2009; Salganik, 2018; Welser, Smith, Fisher, &amp; Gleave, 2008). The data for this walk-through is a summary of log-trace data, namely, the number of minutes students spent on the course. Thus, while this data is rich, you can imagine even more complex sources of log-trace data (i.e. timestamps associated with when students started and stopped accessing the course!). 6.1.3 Data source #3: Achievement-related and gradebook data This is a common source of data, namely, one associated with graded assignments students completed. In this walkthrough, we just examine students’ final grade. 6.1.4 Data source #4: Discussion board data Discussion board data is both rich and unstructured, in that it is primarily in the form of written text. We collected discussion board data, too, and highlight this as a potentially very rich data source. 6.2 Processing the data This analysis uses R packages, which are collections of R code that help users code more efficiently, as you wil recall from Chapter INTRODUCTORY. We load these packages with the function library. In particular, the packages we’ll use will help us load Excel files, organize the structure of the data, work with dates in the data, and navigate file directories. library(readxl) library(tidyverse) library(lubridate) library(here) library(dataedu) library(apaTables) library(sjPlot) This code chunk loads the log trace data using the read_csv function. Note that we call read_csv three times, once for each of the three logtrace datasets. We assign each of the datasets a name using &lt;-. # Gradebook and log-trace data for F15 and S16 semesters s12_course_data &lt;- read_csv(here( &quot;data&quot;, &quot;online-science-motivation&quot;, &quot;raw&quot;, &quot;s12-course-data.csv&quot; )) # Pre-survey for the F15 and S16 semesters s12_pre_survey &lt;- read_csv(here( &quot;data&quot;, &quot;online-science-motivation&quot;, &quot;raw&quot;, &quot;s12-pre-survey.csv&quot; )) # Log-trace data for F15 and S16 semesters - this is for time spent s12_time_spent &lt;- read_csv(here( &quot;data&quot;, &quot;online-science-motivation&quot;, &quot;raw&quot;, &quot;s12-course-minutes.csv&quot; )) 6.3 Viewing the data Now that we’ve successfully loaded all three logtrace datasets, we can visually inspect the data by typing the names that we assigned to each dataset. s12_pre_survey ## # A tibble: 1,102 x 16 ## RespondentId StartDate CompletedDate LanguageCode opdata_CourseID ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 426746 2015.08.… &lt;NA&gt; en FrScA-S116-01 ## 2 426775 2015.08.… 2015.08.24 1… en BioA-S116-01 ## 3 427483 2015.08.… &lt;NA&gt; en OcnA-S116-03 ## 4 429883 2015.09.… 2015.09.02 1… en AnPhA-S116-01 ## 5 430158 2015.09.… 2015.09.03 9… en AnPhA-S116-01 ## 6 430161 2015.09.… 2015.09.03 9… en AnPhA-S116-02 ## 7 430162 2015.09.… 2015.09.03 9… en AnPhA-T116-01 ## 8 430167 2015.09.… 2015.09.03 9… en BioA-S116-01 ## 9 430170 2015.09.… 2015.09.03 9… en BioA-T116-01 ## 10 430172 2015.09.… 2015.09.03 9… en PhysA-S116-01 ## # … with 1,092 more rows, and 11 more variables: opdata_username &lt;chr&gt;, ## # Q1MaincellgroupRow1 &lt;dbl&gt;, Q1MaincellgroupRow2 &lt;dbl&gt;, ## # Q1MaincellgroupRow3 &lt;dbl&gt;, Q1MaincellgroupRow4 &lt;dbl&gt;, ## # Q1MaincellgroupRow5 &lt;dbl&gt;, Q1MaincellgroupRow6 &lt;dbl&gt;, ## # Q1MaincellgroupRow7 &lt;dbl&gt;, Q1MaincellgroupRow8 &lt;dbl&gt;, ## # Q1MaincellgroupRow9 &lt;dbl&gt;, Q1MaincellgroupRow10 &lt;dbl&gt; s12_course_data ## # A tibble: 29,711 x 16 ## CourseSectionOr… Bb_UserPK EnrollmentStatus EnrollmentReason Gender ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 2 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 3 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 4 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 5 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 6 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 7 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 8 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 9 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## 10 AnPhA-S116-01 60186 Approved/Enroll… Course Unavaila… M ## # … with 29,701 more rows, and 11 more variables: FinalGradeCEMS &lt;dbl&gt;, ## # Gradebook_Item &lt;chr&gt;, Item_Position &lt;dbl&gt;, Gradebook_Type &lt;chr&gt;, ## # Gradebook_Date &lt;chr&gt;, Grade_Category &lt;chr&gt;, Status &lt;lgl&gt;, ## # Points_Earned &lt;chr&gt;, Points_Attempted &lt;dbl&gt;, Points_Possible &lt;dbl&gt;, ## # last_access_date &lt;time&gt; s12_time_spent ## # A tibble: 598 x 6 ## CourseID CourseSectionID CourseSectionOrigID Bb_UserPK CUPK TimeSpent ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 27 17146 OcnA-S116-01 44638 190682 1383. ## 2 27 17146 OcnA-S116-01 54346 194259 1191. ## 3 27 17146 OcnA-S116-01 57981 196014 3343. ## 4 27 17146 OcnA-S116-01 66740 190463 965. ## 5 27 17146 OcnA-S116-01 67920 191593 4095. ## 6 27 17146 OcnA-S116-01 85355 190104 595. ## 7 27 17146 OcnA-S116-01 85644 190685 1632. ## 8 27 17146 OcnA-S116-01 86349 191713 1601. ## 9 27 17146 OcnA-S116-01 86460 191887 1891. ## 10 27 17146 OcnA-S116-01 87970 194256 3123. ## # … with 588 more rows 6.4 Processing the pre-survey data Often, survey data needs to be processed in order to be (most) useful. Here, we process the self-report items into three scales, for: interest, self-efficacy, and utility value. We do this by Renaming the question variables to something more managable Reversing the response scales on questions 4 and 7 Categorizing each question into a measure Computing the mean of each measure Let’s take these steps in order: Rename the question columns to something much simpler: s12_pre_survey &lt;- s12_pre_survey %&gt;% # Rename the qustions something easier to work with because R is case sensitive # and working with variable names in mix case is prone to error rename( q1 = Q1MaincellgroupRow1, q2 = Q1MaincellgroupRow2, q3 = Q1MaincellgroupRow3, q4 = Q1MaincellgroupRow4, q5 = Q1MaincellgroupRow5, q6 = Q1MaincellgroupRow6, q7 = Q1MaincellgroupRow7, q8 = Q1MaincellgroupRow8, q9 = Q1MaincellgroupRow9, q10 = Q1MaincellgroupRow10 ) %&gt;% # Convert all question responses to numeric mutate_at(vars(q1:q10), list( ~ as.numeric(.))) Next we’ll reverse the scale of the survey responses on questions 4 and 7 so the responses for all questions can be interpreted in the same way. Rather than write a lot of code once to reverse the scales for question 4 then writing it again to reverse the scales on question 7, we’ll build a function that does that job for us. Then we’ll use the same function for question 4 and question 7. This will result in much less code, plus it will make it easier for us to change in the future. # This part of the code is where we write the function: # Function for reversing scales reverse_scale &lt;- function(question) { # Reverses the response scales for consistency # Args: # question: survey question # Returns: a numeric converted response # Note: even though 3 is not transformed, case_when expects a match for all # possible conditions, so it&#39;s best practice to label each possible input # and use TRUE ~ as the final statement returning NA for unexpected inputs x &lt;- case_when( question == 1 ~ 5, question == 2 ~ 4, question == 4 ~ 2, question == 5 ~ 1, question == 3 ~ 3, TRUE ~ NA_real_ ) x } # And here&#39;s where we use that function to reverse the scales # Reverse scale for questions 4 and 7 s12_pre_survey &lt;- s12_pre_survey %&gt;% mutate(q4 = reverse_scale(q4), q7 = reverse_scale(q7)) We’ll accomplish the last two steps in one chunk of code. First we’ll create a column called measure and we’ll fill that column with one of three question categories: int: interest uv: utility value pc: self efficacy After that we’ll find the mean response of each category using mean function. # Add measure variable s12_measure_mean &lt;- s12_pre_survey %&gt;% # Gather questions and responses pivot_longer(cols = q1:q10, names_to = &quot;question&quot;, values_to = &quot;response&quot;) %&gt;% # Here&#39;s where we make the column of question categories mutate( measure = case_when( question %in% c(&quot;q1&quot;, &quot;q4&quot;, &quot;q5&quot;, &quot;q8&quot;, &quot;q10&quot;) ~ &quot;int&quot;, question %in% c(&quot;q2&quot;, &quot;q6&quot;, &quot;q9&quot;) ~ &quot;uv&quot;, question %in% c(&quot;q3&quot;, &quot;q7&quot;) ~ &quot;pc&quot;, TRUE ~ NA_character_ ) ) %&gt;% group_by(measure) %&gt;% summarise(# Here&#39;s where we compute the mean of the responses # Mean response for each measure mean_response = mean(response, na.rm = TRUE), # Percent of each measure that had NAs in the response field percent_NA = mean(is.na(response))) s12_measure_mean ## # A tibble: 3 x 3 ## measure mean_response percent_NA ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 int 4.26 0.171 ## 2 pc 3.65 0.170 ## 3 uv 3.76 0.170 We will use a similar process later to calculate these variables’ correlations. 6.5 Processing the course data We also can process the course data in order to create new variables which we can use in analyses. This led to pulling out the subject, semester, and section from the course ID; variables that we can use later on. # split course section into components s12_course_data &lt;- s12_course_data %&gt;% separate( col = CourseSectionOrigID, into = c(&quot;subject&quot;, &quot;semester&quot;, &quot;section&quot;), sep = &quot;-&quot;, remove = FALSE ) 6.6 Joining the data To join the course data and pre-survey data, we need to create similar keys. In other words, our goal here is to have one variable that matches across both datasets, so that we can merge the datasets on the basis of that variable. For these data, both have variables for the course and the student, though they have different names in each. Our first goal will be to rename two variables in each of our datasets so that they will match. One variable will correspond to the course, and the other will correspond to the student. We are not changing anything in the data itself at this step - instead, we are just cleaning it up so that we can look at the data all in one place. Let’s start with the pre-survey data. We will rename RespondentID and opdata_CourseID to be student_id and course_id, respectively. s12_pre_survey &lt;- s12_pre_survey %&gt;% rename(student_id = RespondentId, course_id = opdata_CourseID) s12_pre_survey ## # A tibble: 1,102 x 16 ## student_id StartDate CompletedDate LanguageCode course_id opdata_username ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 426746 2015.08.… &lt;NA&gt; en FrScA-S1… _80624_1 ## 2 426775 2015.08.… 2015.08.24 1… en BioA-S11… _80623_1 ## 3 427483 2015.08.… &lt;NA&gt; en OcnA-S11… _82588_1 ## 4 429883 2015.09.… 2015.09.02 1… en AnPhA-S1… _80623_1 ## 5 430158 2015.09.… 2015.09.03 9… en AnPhA-S1… _80624_1 ## 6 430161 2015.09.… 2015.09.03 9… en AnPhA-S1… _80624_1 ## 7 430162 2015.09.… 2015.09.03 9… en AnPhA-T1… _80624_1 ## 8 430167 2015.09.… 2015.09.03 9… en BioA-S11… _80624_1 ## 9 430170 2015.09.… 2015.09.03 9… en BioA-T11… _80624_1 ## 10 430172 2015.09.… 2015.09.03 9… en PhysA-S1… _80624_1 ## # … with 1,092 more rows, and 10 more variables: q1 &lt;dbl&gt;, q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, ## # q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, q6 &lt;dbl&gt;, q7 &lt;dbl&gt;, q8 &lt;dbl&gt;, q9 &lt;dbl&gt;, q10 &lt;dbl&gt; Looks better now! Let’s proceed to the course data. Our goal is to rename two variables that correspond to the course and the student so that we can match with the other variables we just created for the pre-survey data. s12_course_data &lt;- s12_course_data %&gt;% rename(student_id = Bb_UserPK, course_id = CourseSectionOrigID) Now that we have two variables that are consistent across both datasets - we have called them “course_id” and “student_id” - we can join these using the dplyr function, left_join(). Let’s save our joined data as a new object called “dat.” dat &lt;- left_join(s12_course_data, s12_pre_survey, by = c(&quot;student_id&quot;, &quot;course_id&quot;)) dat ## # A tibble: 29,711 x 33 ## course_id subject semester section student_id EnrollmentStatus ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 2 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 3 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 4 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 5 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 6 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 7 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 8 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 9 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 10 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## # … with 29,701 more rows, and 27 more variables: EnrollmentReason &lt;chr&gt;, ## # Gender &lt;chr&gt;, FinalGradeCEMS &lt;dbl&gt;, Gradebook_Item &lt;chr&gt;, ## # Item_Position &lt;dbl&gt;, Gradebook_Type &lt;chr&gt;, Gradebook_Date &lt;chr&gt;, ## # Grade_Category &lt;chr&gt;, Status &lt;lgl&gt;, Points_Earned &lt;chr&gt;, ## # Points_Attempted &lt;dbl&gt;, Points_Possible &lt;dbl&gt;, last_access_date &lt;time&gt;, ## # StartDate &lt;chr&gt;, CompletedDate &lt;chr&gt;, LanguageCode &lt;chr&gt;, ## # opdata_username &lt;chr&gt;, q1 &lt;dbl&gt;, q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, ## # q6 &lt;dbl&gt;, q7 &lt;dbl&gt;, q8 &lt;dbl&gt;, q9 &lt;dbl&gt;, q10 &lt;dbl&gt; Just one more data frame to merge: s12_time_spent &lt;- s12_time_spent %&gt;% rename(student_id = Bb_UserPK, course_id = CourseSectionOrigID) s12_time_spent &lt;- s12_time_spent %&gt;% mutate(student_id = as.integer(student_id)) dat &lt;- dat %&gt;% left_join(s12_time_spent, by = c(&quot;student_id&quot;, &quot;course_id&quot;)) Note that they’re now combined, even though the course data has many more rows: The pre_survey data has been joined for each student by course combination. We have a pretty large data frame! Let’s take a quick look. dat ## # A tibble: 29,711 x 37 ## course_id subject semester section student_id EnrollmentStatus ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 2 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 3 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 4 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 5 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 6 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 7 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 8 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 9 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## 10 AnPhA-S1… AnPhA S116 01 60186 Approved/Enroll… ## # … with 29,701 more rows, and 31 more variables: EnrollmentReason &lt;chr&gt;, ## # Gender &lt;chr&gt;, FinalGradeCEMS &lt;dbl&gt;, Gradebook_Item &lt;chr&gt;, ## # Item_Position &lt;dbl&gt;, Gradebook_Type &lt;chr&gt;, Gradebook_Date &lt;chr&gt;, ## # Grade_Category &lt;chr&gt;, Status &lt;lgl&gt;, Points_Earned &lt;chr&gt;, ## # Points_Attempted &lt;dbl&gt;, Points_Possible &lt;dbl&gt;, last_access_date &lt;time&gt;, ## # StartDate &lt;chr&gt;, CompletedDate &lt;chr&gt;, LanguageCode &lt;chr&gt;, ## # opdata_username &lt;chr&gt;, q1 &lt;dbl&gt;, q2 &lt;dbl&gt;, q3 &lt;dbl&gt;, q4 &lt;dbl&gt;, q5 &lt;dbl&gt;, ## # q6 &lt;dbl&gt;, q7 &lt;dbl&gt;, q8 &lt;dbl&gt;, q9 &lt;dbl&gt;, q10 &lt;dbl&gt;, CourseID &lt;dbl&gt;, ## # CourseSectionID &lt;dbl&gt;, CUPK &lt;dbl&gt;, TimeSpent &lt;dbl&gt; It looks like we have nearly 30,000 observations from 30 variables. There is one last step to take. Were we interested in a fine-grained analysis of how students performed (according to the teacher) on different assignments (see the Gradebook_Item column), we would keep all (29,711 rows of) the data. But, our goal (for now) is more modest: to calculate the percentage of points students earned as a measure of their final grade (noting that the teacher may have assigned a different grade–or weighted their grades in ways not reflected through the points). dat &lt;- dat %&gt;% group_by(student_id, course_id) %&gt;% mutate(Points_Earned = as.integer(Points_Earned)) %&gt;% summarize( total_points_possible = sum(Points_Possible, na.rm = TRUE), total_points_earned = sum(Points_Earned, na.rm = TRUE) ) %&gt;% mutate(percentage_earned = total_points_earned / total_points_possible) %&gt;% ungroup() %&gt;% left_join(dat) # note that we join this back to the original data frame to retain all of the variables 6.7 Finding distinct cases at the student-level This last step calculated a new column, for the percentage of points each student earned. That value is the same for the same student (an easy way we would potentially use to check this is View(), i.e., View(dat)). But–because we are not carrying out a finer-grained analysis using the Gradebook_Item–the duplicate rows are not necessary. We only want variables at the student-level (and not at the level of different gradebook items). We can do this using the distinct() function. This function takes the name of the data frame and the name of the variables used to determine what counts as a unique case. Another thing to note about distinct() is that it will only return the variable(s) (we note that you can pass more than one variable to distinct()) you used to determine uniqueness, unless you include the argument .keep_all = TRUE. For the sake of making it very easy to view the output, we omit this argument (only for now). Were we to run distinct(dat, Gradebook_Item), what do you think would be returned? distinct(dat, Gradebook_Item) ## # A tibble: 222 x 1 ## Gradebook_Item ## &lt;chr&gt; ## 1 POINTS EARNED &amp; TOTAL COURSE POINTS ## 2 WORK ATTEMPTED ## 3 0-1.1: Intro Assignment - Send a Message to Your Instructor ## 4 0-1.2: Intro Assignment - DB #1 ## 5 0-1.3: Intro Assignment - Submitting Files ## 6 1-1.1: Lesson 1-1 Graphic Organizer ## 7 1-2.1: Explore a Career Assignment ## 8 1-2.2: Explore a Career DB #2 ## 9 PROGRESS CHECK 1 @ 02-18-16 ## 10 1-2.3: Lesson 1-2 Graphic Organizer ## # … with 212 more rows What is every distinct gradebook item is what is returned. You might be wondering (as we were) whether some gradebook items have the same values across courses; we can return the unique combination of courses and gradebook items by simply adding another variable to distinct(): distinct(dat, course_id, Gradebook_Item) ## # A tibble: 1,269 x 2 ## course_id Gradebook_Item ## &lt;chr&gt; &lt;chr&gt; ## 1 FrScA-S216-02 POINTS EARNED &amp; TOTAL COURSE POINTS ## 2 FrScA-S216-02 WORK ATTEMPTED ## 3 FrScA-S216-02 0-1.1: Intro Assignment - Send a Message to Your Instructor ## 4 FrScA-S216-02 0-1.2: Intro Assignment - DB #1 ## 5 FrScA-S216-02 0-1.3: Intro Assignment - Submitting Files ## 6 FrScA-S216-02 1-1.1: Lesson 1-1 Graphic Organizer ## 7 FrScA-S216-02 1-2.1: Explore a Career Assignment ## 8 FrScA-S216-02 1-2.2: Explore a Career DB #2 ## 9 FrScA-S216-02 PROGRESS CHECK 1 @ 02-18-16 ## 10 FrScA-S216-02 1-2.3: Lesson 1-2 Graphic Organizer ## # … with 1,259 more rows It looks like a lot of gradebook items were repeated - likely across the different sections of the same course (we would be curious to hear what you find if you investigate this!). Let’s use what we just did, but to find the unique values at the student-level. Thus, instead of exploring unique gradebook items, we will explore unique students (still accounting for the course, as students could enroll in more than one course.) This time, we will add the keep_all = TRUE argument. dat &lt;- distinct(dat, course_id, student_id, .keep_all = TRUE) This is a much smaller data frame - with one row for each sudnet in the course (instead of the 29,701 rows which we would be interested in were we analyzing this data at the level of specific students’ grades for specific gradebook items). Now that our data are ready to go, we can start to ask some questions of the data, 6.8 Visualizations and Models 6.8.1 The relationship between time spent on course and percentage of points earned One thing we might be wondering is how time spent on course is related to students’ final grade. We note that ggplot2, which we use to create these plots, is discussed further in chapter XXX. dat %&gt;% ggplot(aes(x = TimeSpent, y = percentage_earned)) + # this tells ggplot2 what variables to map to what feature of a plot, here, x- anx y-axis locations geom_point() + # creates a point with x- and y-axis coordinates specified above theme_dataedu() There appears to be some relationship. What if we added a line of best fit - a linear model? dat %&gt;% ggplot(aes(x = TimeSpent, y = percentage_earned)) + geom_point() + # same as above geom_smooth(method = &quot;lm&quot;) + # this adds a line of best fit; method = &quot;lm&quot; tells ggplot2 to make the line linear, whereas the default is a smooth like theme_dataedu() So, it appeares that the more time students spent on the course, the more points they earned. 6.9 Linear model (regression) We can find out exactly what the relationship is using a linear model. We also discuss linear models in walkthrough XXX. Here, we predict percentage_earned, or the percentage of the total points that are possible for a student to earn. Here, percentage earned is the dependent, or y-variable, and so we enter it first, after the lm() command, before the tilde (~) symbol. To the right of the tilde is one independent variable, TimeSpent, or the time that students spent on the course. We also pass the data frame, dat. At this point, we’re ready to run the model. Let’s run this line of code and save the results to an object - we chose m_linear, but any name will work, as well as the summary() function on the output. m_linear &lt;- lm(percentage_earned ~ TimeSpent, data = dat) Another way that we can generate table output is with a function from the sjPlot package, tab_model. sjPlot::tab_model(m_linear) percentage earned Predictors Estimates CI p (Intercept) 0.62 0.59 – 0.65 &lt;0.001 TimeSpent 0.00 0.00 – 0.00 &lt;0.001 Observations 598 R2 / R2 adjusted 0.191 / 0.189 This will work well for R Markdown documents (or simply to interpet the model in R). If you want to save the model for use in a Word document, the apaTables package may be helpful; just pass the name of the regression model, like we did with sjPlot::tab_model(), as well as a file name that ends in .doc to the filename argument, i.e.: apaTables::apa.reg.table(m_linear) ## ## ## Regression results using percentage_earned as the criterion ## ## ## Predictor b b_95%_CI beta beta_95%_CI sr2 sr2_95%_CI r ## (Intercept) 0.62** [0.59, 0.65] ## TimeSpent 0.00** [0.00, 0.00] 0.44 [0.36, 0.51] .19 [.14, .24] .44** ## ## ## ## Fit ## ## ## R2 = .191** ## 95% CI[.14,.24] ## ## ## Note. A significant b-weight indicates the beta-weight and semi-partial correlation are also significant. ## b represents unstandardized regression weights. beta indicates the standardized regression weights. ## sr2 represents the semi-partial correlation squared. r represents the zero-order correlation. ## Square brackets are used to enclose the lower and upper limits of a confidence interval. ## * indicates p &lt; .05. ** indicates p &lt; .01. ## Helpfully, you can save the output to a Word document, by adding a filename argument: apaTables::apa.reg.table(m_summary, filename = &quot;regression-table-output.doc&quot;) You might be wondering what else the apaTables package does; we encourage you to read more about the package here: https://cran.r-project.org/web/packages/apaTables/index.html. The vignette is especially helpful. One function that may be useful for writing manuscripts is the following function for creating correlation tables; the function takes, as an input, a data frame with the variables for which you wish to calculate correlations. We will create the same measures (based on the survey items) that we used earlier to understand how they relate to one another: survey_responses &lt;- s12_pre_survey %&gt;% # Gather questions and responses pivot_longer(cols = q1:q10, names_to = &quot;question&quot;, values_to = &quot;response&quot;) %&gt;% mutate( # Here&#39;s where we make the column of question categories measure = case_when( question %in% c(&quot;q1&quot;, &quot;q4&quot;, &quot;q5&quot;, &quot;q8&quot;, &quot;q10&quot;) ~ &quot;int&quot;, question %in% c(&quot;q2&quot;, &quot;q6&quot;, &quot;q9&quot;) ~ &quot;uv&quot;, question %in% c(&quot;q3&quot;, &quot;q7&quot;) ~ &quot;pc&quot;, TRUE ~ NA_character_ ) ) %&gt;% group_by(student_id, measure) %&gt;% summarise(# Here&#39;s where we compute the mean of the responses # Mean response for each measure mean_response = mean(response, na.rm = TRUE)) %&gt;% filter(!is.na(mean_response)) %&gt;% spread(measure, mean_response) survey_responses ## # A tibble: 920 x 4 ## # Groups: student_id [920] ## student_id int pc uv ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 429883 1.8 3 1 ## 2 430158 NA NA 1 ## 3 430161 NA 1 NA ## 4 431821 4.4 3.5 4 ## 5 431864 4.4 5 3 ## 6 431889 3.8 3 3.67 ## 7 431890 5 4 3.67 ## 8 431909 4 3 3 ## 9 431949 3.8 3.5 3.33 ## 10 431956 4 4 3.67 ## # … with 910 more rows Now that we’ve prepared the survey responses, we can use the apa.cor.table() function: survey_responses %&gt;% apa.cor.table() ## ## ## Means, standard deviations, and correlations with confidence intervals ## ## ## Variable M SD 1 2 3 ## 1. student_id 461015.15 29419.03 ## ## 2. int 4.26 0.63 -.11** ## [-.17, -.05] ## ## 3. pc 3.65 0.72 -.09** .59** ## [-.15, -.02] [.55, .63] ## ## 4. uv 3.76 0.81 -.09** .61** .52** ## [-.15, -.02] [.57, .65] [.47, .56] ## ## ## Note. M and SD are used to represent mean and standard deviation, respectively. ## Values in square brackets indicate the 95% confidence interval. ## The confidence interval is a plausible range of population correlations ## that could have caused the sample correlation (Cumming, 2014). ## * indicates p &lt; .05. ** indicates p &lt; .01. ## The time spent variable is on a very large scale (minutes); what if we transform it to represent the number of hours that students spent on the course? Let’s use the mutate() function we used earlier. We’ll end the variable name in _hours, to represent what this variable means. # creating a new variable for the amount of time spent in hours dat &lt;- dat %&gt;% mutate(TimeSpent_hours = TimeSpent / 60) # the same linear model as above, but with the TimeSpent variable in hours m_linear_1 &lt;- lm(percentage_earned ~ TimeSpent_hours, data = dat) # viewing the output of the linear model sjPlot::tab_model(m_linear_1) percentage earned Predictors Estimates CI p (Intercept) 0.62 0.59 – 0.65 &lt;0.001 TimeSpent_hours 0.00 0.00 – 0.01 &lt;0.001 Observations 598 R2 / R2 adjusted 0.191 / 0.189 The scale still does not seem quite right. What if we standardized the variable to have a mean of zero and a standard deviation of one? # this is to standardize the TimeSpent variable to have a mean of zero and a standard deviation of 1 dat &lt;- dat %&gt;% mutate(TimeSpent_std = scale(TimeSpent)) # the same linear model as above, but with the TimeSpent variable standardized m_linear_2 &lt;- lm(percentage_earned ~ TimeSpent_std, data = dat) # viewing the output of the linear model sjPlot::tab_model(m_linear_2) percentage earned Predictors Estimates CI p (Intercept) 0.76 0.74 – 0.78 &lt;0.001 TimeSpent_std 0.11 0.09 – 0.13 &lt;0.001 Observations 598 R2 / R2 adjusted 0.191 / 0.189 That seems to make more sense. However, there is a different interpretation now for the time spent variable: for every one standard deviation increase in the amount of time spent on the course, the percentage of points a student earns increases by .11, or 11 percentage points. Let’s extend our regression model: what other variables may matter? Perhaps there are differences based on the subject of the course. We can add subject as a variable easily:it l # a linear model with the subject added # independent variables, such as TimeSpent_std and subject, can simply be separated with a plus symbol: m_linear_3 &lt;- lm(percentage_earned ~ TimeSpent_std + subject, data = dat) We can use sjPlot::tab_model() once again to view the results: sjPlot::tab_model(m_linear_3) percentage earned Predictors Estimates CI p (Intercept) 0.70 0.66 – 0.73 &lt;0.001 TimeSpent_std 0.12 0.10 – 0.14 &lt;0.001 subject (BioA) -0.00 -0.08 – 0.07 0.940 subject (FrScA) 0.11 0.07 – 0.16 &lt;0.001 subject (OcnA) 0.00 -0.05 – 0.05 0.963 subject (PhysA) 0.18 0.12 – 0.25 &lt;0.001 Observations 598 R2 / R2 adjusted 0.260 / 0.254 It looks like subject FrSc - forensic science - and subject Ocn - oceanography - are associated with a higher percentage of points earned, overall. This indicates that students in those two classes earned higher grades than students in other science classes in this dataset. 6.10 What is next? In the follow-up to this walkthrough (see Chapter XXX), we will focus on visualizing and then modeling the data using an advanced methodological technique, multi-level models. Before we go, let’s save the data we processed. write_csv(dat, &quot;data/online-science-motivation/processed/dat.csv&quot;) "]
]
