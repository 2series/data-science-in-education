# Education Dataset Analysis Pipeline: Walkthrough #1

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, cache = F)
```

## Background 

In the 2015-2016 and 2016-2017 school years, researchers carried out a study on students' motivation to learn in online science classes. The online science classes were part of a statewide online course provider designed to supplement (and not replace) students' enrollment in their local school. For example, students may choose to enroll in an online physics class because one was not offered at their school (or they were not able to take it given their schedule).

The study involved a number of different data sources which were explored to understand students' motivation:

1. A self-report survey for three distinct but related aspects of students' motivation
1. Log-trace data, such as data output from the learning management system
1. Discussion board data
1. Achievement-related (i.e., final grade) data

First, these different data sources will be described in terms of how they were provided by the school.

### Data source #1: Self-report survey 

This was data collected before the start of the course via self-report survey. The survey included 10 items, each corresponding to one of three *measures*, namely, for interest, utility value, and perceived competence:

1.	I think this course is an interesting subject. (Interest)
2.	What I am learning in this class is relevant to my life. (Utility value)
3.	I consider this topic to be one of my best subjects. (Perceived competence)
4.	I am not interested in this course. (Interest - reverse coded)
5.	I think I will like learning about this topic. (Interest)
6.	I think what we are studying in this course is useful for me to know. (Utility value)
7.	I don’t feel comfortable when it comes to answering questions in this area. (Perceived competence)
8.	I think this subject is interesting. (Interest)
9.	I find the content of this course to be personally meaningful. (Utility value)
10.	I’ve always wanted to learn more about this subject. (Interest)

### Data source #2: Log-trace data 

*Log-trace data* is data generated from our interactions with digital technologies, such as archived data from social media postings (see Chapter XXX and XXX). In education, an increasingly common source of log-trace data is that generated from interactions with learning management systems and other digital tools (Lazer et al., 2009; Salganik, 2018; Welser, Smith, Fisher, & Gleave, 2008). The data for this walk-through is a *summary of* log-trace data, namely, the number of minutes students spent on the course. Thus, while this data is rich, you can imagine even more complex sources of log-trace data (i.e. timestamps associated with when students started and stopped accessing the course!).

### Data source #3: Achievement-related and gradebook data

This is a common source of data, namely, one associated with graded assignments students completed. In this walkthrough, we just examine students' final grade.

### Data source #4: Discussion board data

Discussion board data is both rich and unstructured, in that it is primarily in the form of written text. We collected discussion board data, too, and highlight this as a potentially very rich data source.

## Processing the data

This analysis uses R packages, which are collections of R code that help users code more efficiently, as you wil recall from Chapter INTRODUCTORY. We load these packages with the function `library`. In particular, the packages we'll use will help us load Excel files, organize the structure of the data, work with dates in the data, and navigate file directories. 

```{r, loading-packages}
library(readxl)
library(tidyverse)
library(lubridate)
library(here)
```

This code chunk loads the log trace data using the `read_csv` function. Note that we call `read_csv` three times, once for each of the three logtrace datasets. We assign each of the datasets a name using `<-`.

```{r}
# Gradebook and log-trace data for F15 and S16 semesters
s12_course_data <- read_csv(
  here(
    "data", 
    "online-science-motivation", 
    "raw", 
    "s12-course-data.csv"
  )
)

# Pre-survey for the F15 and S16 semesters
s12_pre_survey  <- read_csv(
  here(
    "data", 
    "online-science-motivation", 
    "raw", 
    "s12-pre-survey.csv"
  )
) 

# Log-trace data for F15 and S16 semesters - this is for time spent
s12_time_spent <- read_csv(
  here(
    "data", 
    "online-science-motivation", 
    "raw", 
    "s12-course-minutes.csv"
  )
)
```

## Viewing the data

Now that we've successfully loaded all three logtrace datasets, we can visually inspect the data by typing the names that we assigned to each dataset. 

```{r}
s12_pre_survey 
s12_course_data
s12_time_spent
```

## Processing the pre-survey data

Often, survey data needs to be processed in order to be (most) useful. Here, we process the self-report items into three scales, for: interest, self-efficacy, and utility value. We do this by 

- Renaming the question variables to something more managable 
- Reversing the response scales on questions 4 and 7 
- Categorizing each question into a measure 
- Computing the mean of each measure 

Let's take these steps in order: 

1. Rename the question columns to something much simpler: 

```{r}
s12_pre_survey  <- s12_pre_survey  %>%
  # Rename the qustions something easier to work with because R is case sensitive
  # and working with variable names in mix case is prone to error
  rename(q1 = Q1MaincellgroupRow1,
         q2 = Q1MaincellgroupRow2,
         q3 = Q1MaincellgroupRow3,
         q4 = Q1MaincellgroupRow4,
         q5 = Q1MaincellgroupRow5,
         q6 = Q1MaincellgroupRow6,
         q7 = Q1MaincellgroupRow7,
         q8 = Q1MaincellgroupRow8,
         q9 = Q1MaincellgroupRow9,
         q10 = Q1MaincellgroupRow10) %>% 
  # Convert all question responses to numeric
  mutate_at(vars(q1:q10), funs(as.numeric))
```

2. Next we'll reverse the scale of the survey responses on questions 4 and 7 so the responses for all questions can be interpreted in the same way. Rather than write a lot of code once to reverse the scales for question 4 then writing it again to reverse the scales on question 7, we'll build a function that does that job for us. Then we'll use the same function for question 4 and question 7. This will result in much less code, plus it will make it easier for us to change in the future. 

```{r}
# This part of the code is where we write the function:

# Function for reversing scales 
reverse_scale <- function(question) {
  # Reverses the response scales for consistency 
  #   Args: 
  #     question: survey question 
  #   Returns: a numeric converted response
  # Note: even though 3 is not transformed, case_when expects a match for all
  # possible conditions, so it's best practice to label each possible input
  # and use TRUE ~ as the final statement returning NA for unexpected inputs
  x <- case_when(question == 1 ~ 5, 
                 question == 2 ~ 4,
                 question == 4 ~ 2,
                 question == 5 ~ 1,
                 question == 3 ~ 3,
                 TRUE ~ NA_real_)
  x
}

# And here's where we use that function to reverse the scales

# Reverse scale for questions 4 and 7
s12_pre_survey <- s12_pre_survey %>%
  mutate(q4 = reverse_scale(q4),
         q7 = reverse_scale(q7))
```

3. We'll accomplish the last two steps in one chunk of code. First we'll create a column called `measure` and we'll fill that column with one of three question categories: 

- `int`: interest
- `uv`: utility value 
- `pc`: self efficacy

After that we'll find the mean response of each category using `mean` function.

```{r}
# Add measure variable 
s12_measure_mean <- s12_pre_survey %>% 
  # Gather questions and responses 
  gather(question, response, c(q1:q10)) %>% 
  mutate(
    # Here's where we make the column of question categories 
    measure = case_when(
      question %in% c("q1", "q4", "q5", "q8", "q10") ~ "int", 
      question %in% c("q2", "q6", "q9") ~ "uv", 
      question %in% c("q3", "q7") ~ "pc", 
      TRUE ~ NA_character_
    )) %>% 
  group_by(measure) %>% 
  summarise(
    # Here's where we compute the mean of the responses 
    # Mean response for each measure
    mean_response = mean(response, na.rm = TRUE), 
    # Percent of each measure that had NAs in the response field
    percent_NA = mean(is.na(response))
  )

s12_measure_mean
```

We will use a similar process later to calculate these variables' correlations. 

## Processing the course data

We also can process the course data in order to create new variables which we can use in analyses. This led to pulling out the subject, semester, and section from the course ID; variables that we can use later on.

```{r}
# split course section into components
s12_course_data <- s12_course_data %>%
  separate(col = CourseSectionOrigID,
           into = c('subject', 'semester', 'section'),
           sep = '-',
           remove = FALSE)
```

## Joining the data

To join the course data and pre-survey data, we need to create similar *keys*. In other words, our goal here is to have one variable that matches across both datasets, so that we can merge the datasets on the basis of that variable. 

For these data, both have variables for the course and the student, though they have different names in each. Our first goal will be to rename two variables in each of our datasets so that they will match. One variable will correspond to the course, and the other will correspond to the student. We are not changing anything in the data itself at this step - instead, we are just cleaning it up so that we can look at the data all in one place.

Let's start with the pre-survey data. We will rename RespondentID and opdata_CourseID to be student_id and course_id, respectively.

```{r}
s12_pre_survey <- s12_pre_survey %>% 
  rename(student_id = RespondentId,
         course_id = opdata_CourseID)

s12_pre_survey
```

Looks better now!

Let's proceed to the course data. Our goal is to rename two variables that correspond to the course and the student so that we can match with the other variables we just created for the pre-survey data.

```{r}
s12_course_data <- s12_course_data %>% 
  rename(student_id = Bb_UserPK,
         course_id = CourseSectionOrigID)
```

Now that we have two variables that are consistent across both datasets - we have called them "course_id" and "student_id" -  we can join these using the **dplyr** function, `left_join()`. 
Let's save our joined data as a new object called "dat."

```{r}
dat <- left_join(s12_course_data, 
                 s12_pre_survey, 
                 by = c("student_id", "course_id"))

dat
```

Just one more data frame to merge:

```{r}
s12_time_spent <- s12_time_spent %>%
  rename(student_id = Bb_UserPK, 
         course_id = CourseSectionOrigID)

s12_time_spent <- s12_time_spent %>%
  mutate(student_id = as.integer(student_id))

dat <- dat %>% 
  left_join(s12_time_spent, 
            by = c("student_id", "course_id"))
```

Note that they're now combined, even though the course data has many more rows: The pre_survey data has been joined for each student by course combination.

We have a pretty large data frame! Let's take a quick look.

```{r}
dat
```

It looks like we have nearly 30,000 observations from 30 variables.

There is one last step to take. Were we interested in a fine-grained analysis of how students performed (according to the teacher) on different assignments (see the `Gradebook_Item` column), we would keep all (29,711 rows of) the data. But, our goal (for now) is more modest: to calculate the percentage of points students earned as a measure of their final grade (noting that the teacher may have assigned a different grade--or weighted their grades in ways not reflected through the points).

```{r}
dat <- dat %>% 
  group_by(student_id, course_id) %>% 
  mutate(Points_Earned = as.integer(Points_Earned)) %>% 
  summarize(total_points_possible = sum(Points_Possible, na.rm = TRUE),
            total_points_earned = sum(Points_Earned, na.rm = TRUE)) %>% 
  mutate(percentage_earned = total_points_earned/total_points_possible) %>% 
  ungroup() %>% 
  left_join(dat) # note that we join this back to the original data frame to retain all of the variables
```

Now that our data are ready to go, we can start to ask some questions of the data. 

## Visualizations and Models

### The relationship between time spent on course and percentage of points earned

One thing we might be wondering is how time spent on course is related to students' final grade.

<!-- This is really trivial and obvious; need a new/better relationship -->

```{r}
ggplot(dat, aes(x = TimeSpent, y = percentage_earned)) +
  geom_point() 
```

There appears to be *some* relationship. What if we added a line of best fit - a linear model?

```{r}
ggplot(dat, aes(x = TimeSpent, y = percentage_earned)) +
  geom_point() + 
  geom_smooth(method = "lm")
```

So, it appeares that the more time students spent on the course, the more points they earned.

## Linear model (regression)

We can find out exactly what the relationship is using a linear model. We also discuss linear models in walkthrough XXX.

Here, we predict `percentage_earned`, or the percentage of the total points that
are possible for a student to earn. Here, percentage earned is the dependent, or
*y*-variable, and so we enter it first, after the `lm()` command, before the 
tilde (`~`) symbol. To the right of the tilde is one independent variable, 
`TimeSpent`, or the time that students spent on the course. We also pass the 
data frame, `dat`. At this point, we're ready to run the model. Let's run this
line of code and save the results to an object - we chose `m_linear`, but any 
name will work, as well as the `summary()` function on the output.

```{r}
m_linear <- lm(percentage_earned ~ TimeSpent, data = dat)
sjPlot::tab_model(m_linear)
```

Another way that we can generate table output is with a function from the 
`sjPlot` package, `tab_model`.

```{r}
sjPlot::tab_model(m_linear)
```

This will work well for R Markdown documents (or simply to interpet the model in
R). If you want to save the model for use in a Word document, the 
[apaTables](https://cran.r-project.org/web/packages/apaTables/vignettes/apaTables.html) package may be helpful;
just pass the name of the regression model, like we did with `sjPlot::tab_model()`, 
as well as a file name that ends in `.doc` to the `filename` argument, i.e.: 

```{r, eval = FALSE}
library(apaTables)
apa.reg.table(m_summary, filename = "m_summary_regression_table.doc")
```

<!-- Wonder if we also want to show how to do correlation tables with this package -->

You might be wondering what else the apaTables package does; we encourage you to read more about the package here: https://cran.r-project.org/web/packages/apaTables/index.html. The vignette is especially helpful. One function that may be useful for writing manuscripts is the following function for creating correlation tables; the function takes, as an input, a data frame with the variables for which you wish to calculate correlations. We will create the same measures (based on the survey items) that we used earlier to understand how the percentage of points earned is related to these measures (and how all of the variables relate to one another:

```{r}
s12_measure_mean <- s12_pre_survey %>% 
  # Gather questions and responses 
  gather(question, response, c(q1:q10)) %>% 
  mutate(
    # Here's where we make the column of question categories 
    measure = case_when(
      question %in% c("q1", "q4", "q5", "q8", "q10") ~ "int", 
      question %in% c("q2", "q6", "q9") ~ "uv", 
      question %in% c("q3", "q7") ~ "pc", 
      TRUE ~ NA_character_
    )) %>% 
  
  select(percentage_earned, int, uv, pc)
apa.cor.table()
```

The time spent variable is on a very large scale (minutes); what if we transform it to 
represent the number of hours that students spent on the course into hours? Let's use 
the `mutate()` function we used earlier. We'll end the variable name in `_hours`, 
to represent what this variable means.

```{r}
dat <- dat %>% mutate(TimeSpent_hours = TimeSpent/60)
m_linear_1 <- lm(percentage_datearned ~ TimeSpent_hours, data = dat)
sjPlot::tab_model(m_linear_1)
```

The scale stil does not seem quite right. What if we standardized the variable 
to have a mean of zero and a standard deviation of one?

```{r}
dat <- dat %>% mutate(TimeSpent_std = scale(TimeSpent))
m_linear_2 <- lm(percentage_earned ~ TimeSpent_std, data = dat)
sjPlot::tab_model(m_linear_2)
```

That seems to make more sense - although, there is a different interpretation 
now for the time spent variable: for every one standard deviation increase in 
the amount of time spent on the course, the percentage of points a student earns 
increases by .11, or 11 percentage points. 

Let's extend our regression model: what other variables may matter? Perhaps there
are differences based on the subject of the course. We can add subject as a variable easily:

```{r}
m_linear_3 <- lm(percentage_earned ~ scale(TimeSpent) + subject, data = dat)
```

We can use `sjPlot::tab_model()` once again to view the results:

```{r}
sjPlot::tab_model(m_linear_3)
```

It looks like subject `FrSc` - forensic science - and subject `Ocn` - 
oceanography - are associated with a higher percentage of points earned, overall.

<!-- Not sure the following is very interpretable - cutting unless a better  -->
<!-- interaction term can be specified -->
<!-- Is it possible that the relationship between time spent on the course and the  -->
<!-- percentage of points earned differs as a function of the subject? To find out,  -->
<!-- we can specify an interaction term. An interaction term represents the additional  -->
<!-- impact of two variables both being high - or low - on the dependent variable; in -->
<!-- this case, the percentage of points earned. -->

<!-- ```{r} -->
<!-- m_linear_4 <- lm(percentage_earned ~ scale(TimeSpent)*subject, data = dat) -->
<!-- sjPlot::tab_model(m_linear_4) -->
<!-- ``` -->

<!-- THIS COULD BE THE START OF A NEW WALKTHROUGH HERE -->

## But what about different courses?

Are there course-specific differences in how much time students spend on the 
course as well as in how time spent is related to the percentage of points 
students earned? There are a number of ways to approach this question. Let's use 
our linear model.

Specifically, we can dummy-code the groups, which means that we will add a 
number of variables to our data set.

### The role of dummy codes

We can see how dummy coding works through using the {dummies} package, though, 
as we will see, you often do not need to manually dummy code variables like this.

Let's consider the **iris** data that comes built into R, but, since we are 
fans of the {tidyverse}, we will first change it into a tibble.

```{r}
iris <- as_tibble(iris)
iris
```

As we can see, the `Species` variable is a factor. If we consider how we could 
include a variable such as this in a linear model, things may become a little 
confusing. `Species` seems to be made up of, well, words, such as "setosa." The
common way to approach this is through dummy coding, where you create new 
variables for each of the possible values of `Species` (such as "setosa"). Then, 
these new variables have a value of 1 when the row is associated with that level
(i.e., the first row in the data frame above would have a 1 for a column named 
`setosa`).

Let's return to {dummies}.

How many possible values are there for `Species`? We can check with the `levels`
function.

```{r}
levels(iris$Species)
```

When we run the `dummy()` function on the `Species` variable, we can see that it
returns *three* variables, one for each of the three levels of Species - 
"setosa", "versicolor", and "virginica".

```{r}
dummy(iris$Species) %>%
  head()
```

We can confirm that every row associated with a specific species has a 1 in the 
column it corresponds to. We can do this by binding together the dummy codes and 
the **iris** data and then counting, for each of the three species, how much of 
the rows for each dummy code were coded with a "1".

For example, when the `Species` is "setosa", the variable `Speciessetosa` always 
equals 1 - as is the case for the other species (for their respective variables). 

```{r}
species_dummy_coded <- dummies::dummy(iris$Species)
species_dummy_coded <- as_tibble(species_dummy_coded)
iris_with_dummy_codes <- bind_cols(iris, species_dummy_coded)

iris_with_dummy_codes %>% 
  count(Species, Speciessetosa, Speciesversicolor, Speciesvirginica)
```

Okay, this covers how dummy codes work: but, how do they work when used in a 
model, like the linear model we have been using (with `lm()`)? 

In the context of using `lm()` (and many other functions in R) is that the 
number of levels to be created is always the number of different possible values 
minus one, because each group will be modeled in comparison to the group without
a column, or what is commonly called the reference group. 

Why can every group not simply have their own dummy-coded column? The reason has 
to do with how the dummy codes are used. The purpose of the dummy code is to 
show how different the dependent variable is for all of the observations that 
are in that group (i.e., for all of the flowers that are setosa specimens). In 
order to represent how different those flowers are, they have to be compared to 
something else - and the intercept of the model usually represents this 
"something else." However, if every level of a factor (such as `Species`) is 
dummy-coded, then there would be no cases available to estimate an intercept -
in short, the dummy code would not be compared to anything else. For this 
reason, one group is typically selected as the reference group, that which every 
other group is compared to.

### Using dummy codes

This may be clearer with an example. Let's return to our online science class 
data and consider the effect of being in a specific class in the data set. 
First, let's determine how many classes there are. We can use the `count()` 
function.

```{r}
dat %>% 
  count(course_id) %>% 
  nrow()
```

There are 26 distinct courses.

We will save this output to `m_linear_courses`, where the `dc` stands for 
dummy code. We will keep  the variables we used in our last set of models - 
`TimeSpent` and `subject` - as independent variables.

```{r}
m_linear_dc <- lm(percentage_earned ~ TimeSpent_std + course_id, data = dat)
```

The output will be a bit, well, long, because each group will have its own 
intercept. Here it is:

```{r}
sjPlot::tab_model(m_linear_dc)
```

Wow! That is a lot of effects. In addition to the time spent and subject 
variables, the model estimated the difference, accounting for the effects of 
being a student in a specific class. Let's count how many classes 
there are. If we count the number of classes, we see that there are 25 - and not
26! One has been automatically selected as the reference group, and every other 
class's coefficient represents how different each class is from it. The 
intercept's value of 0.74 represents the percentage of points that students in 
the reference group class, which is automatically the first level of the 
`course_id` variable when it is converted to a factor, "AnPhA-S116-01".

We can easily choose another class to serve as a reference group. Imagine, for 
example, that we want "course_idPhysA-S116-01" to be the reference group. The 
`fct_relevel()` function (which is a part of the {tidyverse} suite of packages) 
makes it easy to do this. 

```{r}
dat <- dat %>% 
  mutate(course_id = fct_relevel(course_id, "course_idPhysA-S116-01"))
```

We can now see that *that* group is no longer listed as an independent variable, 
or a predictor: every coefficient in this model is now in reference to it.

```{r}
m_linear_dc_1 <- lm(percentage_earned ~ TimeSpent_std + course_id, data = dat)
sjPlot::tab_model(m_linear_dc_1)
```

Using dummy codes is very common - they are used in nearly every case in which 
you are using a model (such as a linear model, through `lm()`) and you have 
variables that are factors. A benefit of using `lm()` (and many other functions) 
in R for modeling, such as the `lme4::lmer()` function we discuss later is that 
if you have variables which are not factors, but simply character strings, they 
will be automatically changed to factors when used in a model. This means, for 
instance, that if you have a variable for the subject matter of courses labeled 
"mathematics", "science", "english language" (typed like that!), "social 
studies", and "art", and you include this variable in an `lm()` model, then the 
function will automatically dummy-code these for you. The only essential step 
that is not taken for you is choosing which is the reference group.

We note that there are cases in which *not having a reference group* that the
other, dummy-coded groups are compared to is desired. In such cases, no 
intercept is estimated. This can be done by passing a -1 as the first value 
after the tilde, as follows:

```{r}
m_linear_dc_2 <- lm(percentage_earned ~ -1 + TimeSpent_std + course_id, data = dat)
sjPlot::tab_model(m_linear_dc_2)
```

This does not work in many cases, and it is much more common to dummy-code 
factors, and so we emphasized that in this walkthrough, although want you to 
be aware that it is possible (though uncommon) to estimate a model without an 
intercept.

## Toward multi-level models

Dummy-coding is a very helpful strategy. It is particularly useful with a small 
number of groups (i.e., for estimating the effects of being in one of the five
subjects in the online science data set). With effects such as being a student 
in a particular class, though, the output seems to be less useful: it is hard to
interpret the 25 different effects (and to compare them to the intercept). 

Maybe more important than the challenge of interpreting them, analysts often 
have the goal not of determining the effect of being in a specific class, *per 
se*, but rather to account for the fact that students share a class. This is 
important because linear models (i.e., the model we estimated using `lm()`)
have an assumption that the data points are - apart from sharing levels of the
variables that are used in the model - independent, or not correlated. This is 
what is meant by the "assumption of independence" or of "independently and 
identically distributed" (i.i.d.) residuals (CITE Field). 

Multi-level models are a way to deal with the difficulty of interpreting the 
estimated effects for each of many groups, like classes, and to address the 
assumption of independence. Multi-level models do this by still estimating the 
effect of being a student in each group, but with a key difference from linear 
models: Instead of determining how difference the observations in a group are 
from those in the reference group, the multi-level model "regularizes" the 
difference based on how systematically different the groups are. Groups that are 
comprised of individuals who are consistently different (higher or lower) than 
individuals on average are not regularized very much - their estimated 
difference may be close to the estimate from a multi-level model - whereas 
groups with only a few individuals, or with a lot of variability within 
individuals, would be regularized a lot. In the former case, the multi-level 
model considers there to be strong evidence for a group effect, whereas in the 
latter, the model recognizes that there is less certainty about a group (class)
effect for that particular group. Multi-level models are very common in educational research for cases such as 
this: accounting for the way in which students take the same classes, or even go 
to the same school (see Raudenbush & Bryk, 2002).

The way that a multi-level model does this "regularizing" is by considering the 
groups (and not the data points, in this case) to be samples from a larger 
population of classes. By considering the effects of groups to be samples from a 
larger population, the model is able to use information not only particular to 
each group (as the models created using `lm()`), but also information across all
of the data. Using multi-level models means that the assumption of independence 
can be addressed; their use also means that individual coefficients for 
classes do not need to be included (or interpreted, thankfully!), though they 
are still included in and accounted for in the model. As we describe, the way
that information about the groups is reported is usually in the form of the 
*intraclass correlation coefficient* (ICC), which explains the proportion of 
variation in the dependent variable that the groups explain. Smaller ICCs (such 
as ICCs with values of 0.05, representing 5% of the variation in the dependent 
variable) mean that the groups are not very important; larger ICCs, such as 
ICCs with values of 0.10 or larger (values as high as 0.50 are not uncommon!),
indicate that groups are important and that they have to do with a lot of the 
differences observed in the dependent variable (and that not including them 
may potentially ignore the assumption of independence in a case in which it may 
be important to recognize it - and lead to bias in the results).

That was a lot of technical information about multi-level models; thank you for
sticking with us through it! We wanted to incldue this as multi-level models 
are common: consider how often the data you collect involves students nested 
(or grouped) in classes, or classes nested in schools (or even schools nested 
in districts - you get the picture!). Educational data is complex, and so it is 
not surprising that multi-level models may be encountered in educational data 
science analyses, reports, and articles. 

Fortunately, for all of the complicated details, multi-level models are very 
easy to use in R. Their requires a new package; one of the most common for 
estimating these types of models is **lme4**. We use it very similarly to the 
`lm()` function, but we pass it an additional argument about what the *groups*, 
in the data are. Such a model is often referred to as as "varying intercepts" 
multi-level model, because what is different between the groups is the effect 
of being a student in a class: the intercepts between groups vary.

```{r}
# install.packages("lme4")
library(lme4)
m_course <- lmer(percentage_earned ~ TimeSpent + (1|course_id), data = dat)
summary(m_course)
```

As we mentioned earlier, a common way to understand how much variability is at 
the group level is to calculate the *intra-class* correlation. This value is the
proportion of the variability in the outcome (the *y*-variable) that is 
accounted for solely by the groups identified in the model. There is a useful 
function in the **performance** package for doing this.

```{r}
# install.packages("performance")
library(performance)
icc(m_course)
```

This shows that nearly 17% of the variability in the percentage of points 
students earned can be explained simply by knowing what class they are in. 

There is much more to do with multi-level models. We briefly discuss a 
common extension to the model we just used - although such a model, in which
the effects of being a student in a class are recognized through specifying 
what the groups are, is very common. The extensions is for adding 
additional levels.

The data that we are using is all from one school, and so we cannot estimate a 
"two-level" model. Imagine, however, that instead of 26 classes, we had data 
from students from 230 classes, and that these classes were from 15 schools. We 
could estimate a two-level, varying intercepts (where there are now two groups 
with effects) model very similar to the model we estimated above, but simply 
with another group added for the school. The model will account for the way in 
which the classes are nested within the schools automatically (Bates, Maechler, 
Bolker, & Walker, 2015). 

```{r}
m_course_school <- lmer(percentage_earned ~ TimeSpent + (1|course_id) + (1|school_id), data = dat)
```

Were we to estimate this model (and then use the `icc()` function), we would 
see two ICC values representing the proportion of the variation in the dependent
variable explained by each of the two groups we added - the course *and* the
school. You can add further still levels to the model, as the {lme4} package was
designed for complex multi-level models (and even those with not nested, but
crossed random effects; a topic beyond the scope of this walkthrough, but which 
is described in West, Welch, & Galecki, 2015).

There is much more that can be done with multi-level models. For further 
reading, we recommend the sources we cited in the walkthrough:

- [Raudenbush & Bryk (2002)](https://www.amazon.com/Hierarchical-Linear-Models-Applications-Quantitative/dp/076191904X)
- [West, Welch, & Galecki (2015)](http://www-personal.umich.edu/~bwest/almmussp.html)

Finally, we mention that multi-level models have similarities to the Bayesian 
methods which are becoming more common among some R users - and educational 
data scientists. We recommend the following references for Bayesian methods that
directly reference and build on the models that are able to be estimated using 
{lme4}.

- [Gelman & Hill (2006)](https://www.amazon.com/Analysis-Regression-Multilevel-Hierarchical-Models/dp/052168689X/ref=sr_1_1?keywords=andrew+gelman&qid=1573607019&s=books&sr=1-1)
- [Burkner (2019)](https://cran.r-project.org/web/packages/brms/vignettes/brms_overview.pdf)